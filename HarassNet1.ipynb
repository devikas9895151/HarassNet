{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c434a403-935b-4b5d-a81e-26e5a5e589c8",
   "metadata": {},
   "source": [
    "data loading and Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8878f098-9d61-4d34-a029-93e67c28db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0.0: 0.7306224310041104, 1.0: 1.5840229153405474}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(\"C:\\\\Users\\\\bivin\\\\Favorites\\\\Desktop\\\\internship\\\\train_trimmed.csv\", encoding='utf-8')\n",
    "test_df = pd.read_csv(\"C:\\\\Users\\\\bivin\\\\Favorites\\\\Desktop\\\\internship\\\\test_trimmed.csv\", encoding='utf-8')\n",
    "\n",
    "# Preprocess commentText column\n",
    "def clean_comment_column(df):\n",
    "    df['commentText'] = df['commentText'].astype(str).str.strip()\n",
    "    df['commentText'].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['commentText'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Keep only labels 0 and 1\n",
    "train_df = train_df[train_df['label'].isin([0, 1])]\n",
    "\n",
    "# Clean text columns\n",
    "train_df = clean_comment_column(train_df)\n",
    "test_df = clean_comment_column(test_df)\n",
    "\n",
    "#Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n",
    "class_weight_dict = dict(zip(np.unique(train_df['label']), class_weights))\n",
    "print(f\"Class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47eadb7c-ec15-46f6-8361-88e39ff80470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 5000\n",
      "Label 0 (Non-Harassment): 3406\n",
      "Label 1 (Harassment): 1571\n"
     ]
    }
   ],
   "source": [
    "# Total number of training samples\n",
    "total_samples = len(test_df)\n",
    "\n",
    "# Count of each label\n",
    "label_counts = train_df['label'].value_counts().sort_index()\n",
    "\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Label 0 (Non-Harassment): {label_counts[0]}\")\n",
    "print(f\"Label 1 (Harassment): {label_counts[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71648a-9a74-4aae-846c-cbb31f7c6a84",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f1bf7d7-0fc6-4b28-b1a6-5c465cffaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import emoji\n",
    "\n",
    "# Text cleaning function\n",
    "def deep_clean_text(text):\n",
    "    # Fix common unicode issues\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = emoji.replace_emoji(text, replace='')  # Removes all emojis cleanly\n",
    "    \n",
    "    # Remove mentions (@username)\n",
    "    text = re.sub(r'@[\\w]+', '', text)  # Remove mentions (e.g., @username)\n",
    "    \n",
    "    # Remove URLs (http or https)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs (http:// or www.)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply it to the commentText columns\n",
    "train_df['commentText'] = train_df['commentText'].apply(deep_clean_text)\n",
    "test_df['commentText'] = test_df['commentText'].apply(deep_clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30c5668d-af96-4faf-9564-ae4beb023e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize mBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Tokenize and encode sequences\n",
    "def encode_texts(texts, tokenizer, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf'  # Changed to tf tensors\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "    \n",
    "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb3b1d47-6cf7-4f8d-b4c6-f06403d6aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode train and test data\n",
    "X_train_ids, X_train_masks = encode_texts(train_df['commentText'], tokenizer)\n",
    "X_test_ids, X_test_masks = encode_texts(test_df['commentText'], tokenizer)\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "# Convert y_train to one-hot encoding for categorical crossentropy\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94adea11-6dd2-4ef9-b6ff-977f643da912",
   "metadata": {},
   "source": [
    "fetaure extration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dc8b685-eb45-4c32-aa32-dc4ed0a91625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'cls.predictions.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertModel were not initialized from the PyTorch model and are newly initialized: ['tf_bert_model_6.bert.embeddings.word_embeddings.weight', 'tf_bert_model_6.bert.embeddings.token_type_embeddings.weight', 'tf_bert_model_6.bert.embeddings.position_embeddings.weight', 'tf_bert_model_6.bert.embeddings.LayerNorm.weight', 'tf_bert_model_6.bert.embeddings.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.0.attention.self.query.weight', 'tf_bert_model_6.bert.encoder.layer.0.attention.self.query.bias', 'tf_bert_model_6.bert.encoder.layer.0.attention.self.key.weight', 'tf_bert_model_6.bert.encoder.layer.0.attention.self.key.bias', 'tf_bert_model_6.bert.encoder.layer.0.attention.self.value.weight', 'tf_bert_model_6.bert.encoder.layer.0.attention.self.value.bias', 'tf_bert_model_6.bert.encoder.layer.0.attention.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.0.attention.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.0.intermediate.dense.weight', 'tf_bert_model_6.bert.encoder.layer.0.intermediate.dense.bias', 'tf_bert_model_6.bert.encoder.layer.0.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.0.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.0.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.0.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.1.attention.self.query.weight', 'tf_bert_model_6.bert.encoder.layer.1.attention.self.query.bias', 'tf_bert_model_6.bert.encoder.layer.1.attention.self.key.weight', 'tf_bert_model_6.bert.encoder.layer.1.attention.self.key.bias', 'tf_bert_model_6.bert.encoder.layer.1.attention.self.value.weight', 'tf_bert_model_6.bert.encoder.layer.1.attention.self.value.bias', 'tf_bert_model_6.bert.encoder.layer.1.attention.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.1.attention.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.1.intermediate.dense.weight', 'tf_bert_model_6.bert.encoder.layer.1.intermediate.dense.bias', 'tf_bert_model_6.bert.encoder.layer.1.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.1.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.1.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.1.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.2.attention.self.query.weight', 'tf_bert_model_6.bert.encoder.layer.2.attention.self.query.bias', 'tf_bert_model_6.bert.encoder.layer.2.attention.self.key.weight', 'tf_bert_model_6.bert.encoder.layer.2.attention.self.key.bias', 'tf_bert_model_6.bert.encoder.layer.2.attention.self.value.weight', 'tf_bert_model_6.bert.encoder.layer.2.attention.self.value.bias', 'tf_bert_model_6.bert.encoder.layer.2.attention.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.2.attention.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.2.intermediate.dense.weight', 'tf_bert_model_6.bert.encoder.layer.2.intermediate.dense.bias', 'tf_bert_model_6.bert.encoder.layer.2.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.2.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.2.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.2.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.3.attention.self.query.weight', 'tf_bert_model_6.bert.encoder.layer.3.attention.self.query.bias', 'tf_bert_model_6.bert.encoder.layer.3.attention.self.key.weight', 'tf_bert_model_6.bert.encoder.layer.3.attention.self.key.bias', 'tf_bert_model_6.bert.encoder.layer.3.attention.self.value.weight', 'tf_bert_model_6.bert.encoder.layer.3.attention.self.value.bias', 'tf_bert_model_6.bert.encoder.layer.3.attention.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.3.attention.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.3.intermediate.dense.weight', 'tf_bert_model_6.bert.encoder.layer.3.intermediate.dense.bias', 'tf_bert_model_6.bert.encoder.layer.3.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.3.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.3.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.3.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.4.attention.self.query.weight', 'tf_bert_model_6.bert.encoder.layer.4.attention.self.query.bias', 'tf_bert_model_6.bert.encoder.layer.4.attention.self.key.weight', 'tf_bert_model_6.bert.encoder.layer.4.attention.self.key.bias', 'tf_bert_model_6.bert.encoder.layer.4.attention.self.value.weight', 'tf_bert_model_6.bert.encoder.layer.4.attention.self.value.bias', 'tf_bert_model_6.bert.encoder.layer.4.attention.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.4.attention.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.4.intermediate.dense.weight', 'tf_bert_model_6.bert.encoder.layer.4.intermediate.dense.bias', 'tf_bert_model_6.bert.encoder.layer.4.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.4.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.4.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.4.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.5.attention.self.query.weight', 'tf_bert_model_6.bert.encoder.layer.5.attention.self.query.bias', 'tf_bert_model_6.bert.encoder.layer.5.attention.self.key.weight', 'tf_bert_model_6.bert.encoder.layer.5.attention.self.key.bias', 'tf_bert_model_6.bert.encoder.layer.5.attention.self.value.weight', 'tf_bert_model_6.bert.encoder.layer.5.attention.self.value.bias', 'tf_bert_model_6.bert.encoder.layer.5.attention.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.5.attention.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.5.intermediate.dense.weight', 'tf_bert_model_6.bert.encoder.layer.5.intermediate.dense.bias', 'tf_bert_model_6.bert.encoder.layer.5.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.5.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.5.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.5.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.6.attention.self.query.weight', 'tf_bert_model_6.bert.encoder.layer.6.attention.self.query.bias', 'tf_bert_model_6.bert.encoder.layer.6.attention.self.key.weight', 'tf_bert_model_6.bert.encoder.layer.6.attention.self.key.bias', 'tf_bert_model_6.bert.encoder.layer.6.attention.self.value.weight', 'tf_bert_model_6.bert.encoder.layer.6.attention.self.value.bias', 'tf_bert_model_6.bert.encoder.layer.6.attention.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.6.attention.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.6.intermediate.dense.weight', 'tf_bert_model_6.bert.encoder.layer.6.intermediate.dense.bias', 'tf_bert_model_6.bert.encoder.layer.6.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.6.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.6.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.6.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.7.attention.self.query.weight', 'tf_bert_model_6.bert.encoder.layer.7.attention.self.query.bias', 'tf_bert_model_6.bert.encoder.layer.7.attention.self.key.weight', 'tf_bert_model_6.bert.encoder.layer.7.attention.self.key.bias', 'tf_bert_model_6.bert.encoder.layer.7.attention.self.value.weight', 'tf_bert_model_6.bert.encoder.layer.7.attention.self.value.bias', 'tf_bert_model_6.bert.encoder.layer.7.attention.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.7.attention.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.7.intermediate.dense.weight', 'tf_bert_model_6.bert.encoder.layer.7.intermediate.dense.bias', 'tf_bert_model_6.bert.encoder.layer.7.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.7.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.7.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.7.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.8.attention.self.query.weight', 'tf_bert_model_6.bert.encoder.layer.8.attention.self.query.bias', 'tf_bert_model_6.bert.encoder.layer.8.attention.self.key.weight', 'tf_bert_model_6.bert.encoder.layer.8.attention.self.key.bias', 'tf_bert_model_6.bert.encoder.layer.8.attention.self.value.weight', 'tf_bert_model_6.bert.encoder.layer.8.attention.self.value.bias', 'tf_bert_model_6.bert.encoder.layer.8.attention.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.8.attention.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.8.intermediate.dense.weight', 'tf_bert_model_6.bert.encoder.layer.8.intermediate.dense.bias', 'tf_bert_model_6.bert.encoder.layer.8.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.8.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.8.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.8.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.9.attention.self.query.weight', 'tf_bert_model_6.bert.encoder.layer.9.attention.self.query.bias', 'tf_bert_model_6.bert.encoder.layer.9.attention.self.key.weight', 'tf_bert_model_6.bert.encoder.layer.9.attention.self.key.bias', 'tf_bert_model_6.bert.encoder.layer.9.attention.self.value.weight', 'tf_bert_model_6.bert.encoder.layer.9.attention.self.value.bias', 'tf_bert_model_6.bert.encoder.layer.9.attention.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.9.attention.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.9.intermediate.dense.weight', 'tf_bert_model_6.bert.encoder.layer.9.intermediate.dense.bias', 'tf_bert_model_6.bert.encoder.layer.9.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.9.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.9.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.9.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.10.attention.self.query.weight', 'tf_bert_model_6.bert.encoder.layer.10.attention.self.query.bias', 'tf_bert_model_6.bert.encoder.layer.10.attention.self.key.weight', 'tf_bert_model_6.bert.encoder.layer.10.attention.self.key.bias', 'tf_bert_model_6.bert.encoder.layer.10.attention.self.value.weight', 'tf_bert_model_6.bert.encoder.layer.10.attention.self.value.bias', 'tf_bert_model_6.bert.encoder.layer.10.attention.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.10.attention.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.10.intermediate.dense.weight', 'tf_bert_model_6.bert.encoder.layer.10.intermediate.dense.bias', 'tf_bert_model_6.bert.encoder.layer.10.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.10.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.10.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.10.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.11.attention.self.query.weight', 'tf_bert_model_6.bert.encoder.layer.11.attention.self.query.bias', 'tf_bert_model_6.bert.encoder.layer.11.attention.self.key.weight', 'tf_bert_model_6.bert.encoder.layer.11.attention.self.key.bias', 'tf_bert_model_6.bert.encoder.layer.11.attention.self.value.weight', 'tf_bert_model_6.bert.encoder.layer.11.attention.self.value.bias', 'tf_bert_model_6.bert.encoder.layer.11.attention.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.11.attention.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'tf_bert_model_6.bert.encoder.layer.11.intermediate.dense.weight', 'tf_bert_model_6.bert.encoder.layer.11.intermediate.dense.bias', 'tf_bert_model_6.bert.encoder.layer.11.output.dense.weight', 'tf_bert_model_6.bert.encoder.layer.11.output.dense.bias', 'tf_bert_model_6.bert.encoder.layer.11.output.LayerNorm.weight', 'tf_bert_model_6.bert.encoder.layer.11.output.LayerNorm.bias', 'tf_bert_model_6.bert.pooler.dense.weight', 'tf_bert_model_6.bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define a custom BERT wrapper layer\n",
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        self.bert = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "    def build(self, input_shape):\n",
    "        self.bert = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output  # Return the [CLS] token representation\n",
    "    def get_config(self):\n",
    "        config = super(BertLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "# Define inputs\n",
    "input_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# Use the custom BertLayer\n",
    "bert_layer = BertLayer()\n",
    "pooled_output = BertLayer()([input_ids, attention_mask])\n",
    "\n",
    "# Add classifier layers\n",
    "x = Dense(128, activation='relu')(pooled_output)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(2, activation='softmax')(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd02812-f4bf-4798-bdf5-7a964013bfee",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f5d9466-f3e4-4d84-b36e-150442f1f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=2e-5),\n",
    "    loss='categorical_crossentropy',  # Using categorical since we have one-hot encoded targets\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f26428e-3213-41ba-8f2d-c59796d92535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split train into train and validation (10% for validation)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['label'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6fa74-d846-4602-a199-848ba077d74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m114/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m4:03\u001b[0m 9s/step - accuracy: 0.5619 - loss: 0.6873"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train_ids, X_train_masks], \n",
    "    y_train_onehot,\n",
    "    validation_split=0.1,\n",
    "    epochs=1, class_weight=class_weight_dict,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f00663-bc20-4231-b085-a8edc479f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mbert_model.keras')  # Recommended format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb79b155-e673-40b6-b74d-55319219d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('mbert_model.keras', custom_objects={'BertLayer': BertLayer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb50be-2b7e-4c37-9078-64eaa00d6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract validation data from the training set (using the 10% that was used as validation during training)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get indices for the validation split (last 10% of the training data)\n",
    "val_split = 0.1\n",
    "val_size = int(len(X_train_ids) * val_split)\n",
    "train_size = len(X_train_ids) - val_size\n",
    "\n",
    "# Get validation data\n",
    "X_val_ids = X_train_ids[train_size:]\n",
    "X_val_masks = X_train_masks[train_size:]\n",
    "y_val_true = np.argmax(y_train_onehot[train_size:], axis=1)\n",
    "\n",
    "# Make predictions on validation data\n",
    "val_predictions = model.predict([X_val_ids, X_val_masks])\n",
    "y_val_pred = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_val_true, y_val_pred)\n",
    "precision = precision_score(y_val_true, y_val_pred)\n",
    "recall = recall_score(y_val_true, y_val_pred)\n",
    "f1 = f1_score(y_val_true, y_val_pred)\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\n===== Model Performance on Validation Set =====\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Generate detailed classification report\n",
    "print(\"\\n===== Classification Report =====\")\n",
    "print(classification_report(y_val_true, y_val_pred))\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_val_true, y_val_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Class 0', 'Class 1'],\n",
    "            yticklabels=['Class 0', 'Class 1'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_history.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8feba1a1-14f9-4407-9d09-e3d4a709f679",
   "metadata": {},
   "source": [
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ae1bf-f632-4d9c-ae28-e330639806d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Parameters for the LSTM model\n",
    "max_words = 10000  # Maximum vocabulary size\n",
    "max_len = 128      # Maximum sequence length\n",
    "embedding_dim = 100  # Embedding dimension\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_df['commentText'])\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df['commentText'])\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "\n",
    "# Create training and validation splits (matching what you did with mBERT)\n",
    "val_split = 0.1\n",
    "val_size = int(len(X_train_padded) * val_split)\n",
    "train_size = len(X_train_padded) - val_size\n",
    "\n",
    "X_train_lstm = X_train_padded[:train_size]\n",
    "X_val_lstm = X_train_padded[train_size:]\n",
    "y_train_lstm = y_train_onehot[:train_size]\n",
    "y_val_lstm = y_train_onehot[train_size:]\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(max_words, embedding_dim, input_length=max_len),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_lstm,\n",
    "    y_train_lstm,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_lstm, y_val_lstm)\n",
    ")\n",
    "\n",
    "# Evaluate LSTM model on validation data\n",
    "y_val_lstm_pred = lstm_model.predict(X_val_lstm)\n",
    "y_val_lstm_pred_classes = np.argmax(y_val_lstm_pred, axis=1)\n",
    "y_val_lstm_true = np.argmax(y_val_lstm, axis=1)\n",
    "\n",
    "# Calculate metrics for LSTM model\n",
    "lstm_accuracy = accuracy_score(y_val_lstm_true, y_val_lstm_pred_classes)\n",
    "lstm_precision = precision_score(y_val_lstm_true, y_val_lstm_pred_classes)\n",
    "lstm_recall = recall_score(y_val_lstm_true, y_val_lstm_pred_classes)\n",
    "lstm_f1 = f1_score(y_val_lstm_true, y_val_lstm_pred_classes)\n",
    "\n",
    "print(\"\\n===== LSTM Model Performance on Validation Set =====\")\n",
    "print(f\"Accuracy: {lstm_accuracy:.4f}\")\n",
    "print(f\"Precision: {lstm_precision:.4f}\")\n",
    "print(f\"Recall: {lstm_recall:.4f}\")\n",
    "print(f\"F1 Score: {lstm_f1:.4f}\")\n",
    "\n",
    "print(\"\\n===== LSTM Classification Report =====\")\n",
    "print(classification_report(y_val_lstm_true, y_val_lstm_pred_classes))\n",
    "\n",
    "# Prepare test data for LSTM predictions\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['commentText'])\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# Make predictions on test data with LSTM model\n",
    "lstm_test_predictions = lstm_model.predict(X_test_padded)\n",
    "lstm_predicted_classes = np.argmax(lstm_test_predictions, axis=1)\n",
    "\n",
    "\n",
    "# Compare the two models\n",
    "print(\"\\n===== Model Comparison =====\")\n",
    "print(f\"{'Metric':<15} {'mBERT':<10} {'LSTM':<10}\")\n",
    "print(f\"{'-'*35}\")\n",
    "print(f\"{'Accuracy':<15} {accuracy:<10.4f} {lstm_accuracy:<10.4f}\")\n",
    "print(f\"{'Precision':<15} {precision:<10.4f} {lstm_precision:<10.4f}\")\n",
    "print(f\"{'Recall':<15} {recall:<10.4f} {lstm_recall:<10.4f}\")\n",
    "print(f\"{'F1 Score':<15} {f1:<10.4f} {lstm_f1:<10.4f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "mbert_scores = [accuracy, precision, recall, f1]\n",
    "lstm_scores = [lstm_accuracy, lstm_precision, lstm_recall, lstm_f1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, mbert_scores, width, label='mBERT')\n",
    "plt.bar(x + width/2, lstm_scores, width, label='LSTM')\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Performance Comparison: mBERT vs LSTM')\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a55b14-f94c-415b-bb6f-306965b2f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mBERT predictions\n",
    "mbert_submission_df = test_df.copy()\n",
    "mbert_submission_df['predicted_label'] = np.argmax(model.predict([X_test_ids, X_test_masks]), axis=1)\n",
    "mbert_submission_df.to_excel(\"C:\\\\Users\\\\bivin\\\\Favorites\\\\Desktop\\\\internship\\\\mbert_predictions.xlsx\", index=False)\n",
    "print(\"mBERT Predictions saved to 'mbert_predictions.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e991fb9-beef-4d43-a3d3-583dbc209153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission DataFrame for LSTM\n",
    "lstm_submission_df = test_df.copy()\n",
    "lstm_submission_df['predicted_label'] = lstm_predicted_classes\n",
    "lstm_submission_df.to_excel(\"C:\\\\Users\\\\bivin\\\\Favorites\\\\Desktop\\\\internship\\\\lstm_predictions.xlsx\", index=False)\n",
    "print(\"LSTM Predictions saved to 'lstm_predictions.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69343f-c139-4f71-9e9a-01e6fdb2f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib  # or pickle, or torch.save() depending on your model\n",
    "\n",
    "# Example for a scikit-learn model\n",
    "joblib.dump(lstm_model, 'lstm_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0e675-7709-4249-8f78-3a25d69096fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib  # or pickle, or torch.save() depending on your model\n",
    "\n",
    "# Example for a scikit-learn model\n",
    "joblib.dump(model, 'bert_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0bf68-25e3-46d5-af5c-de1a0524b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn nest_asyncio openpyxl pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fdf2b7-ce58-4adc-a938-590c4b326680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "\n",
    "# Allow running Uvicorn in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load prediction Excel files\n",
    "mbert_df = pd.read_excel(\"C:\\\\Users\\\\bivin\\\\Favorites\\\\Desktop\\\\internship\\\\mbert_predictions.xlsx\")\n",
    "lstm_df = pd.read_excel(\"C:\\\\Users\\\\bivin\\\\Favorites\\\\Desktop\\\\internship\\\\lstm_predictions.xlsx\")\n",
    "\n",
    "# Ensure lowercase and no extra spaces for matching\n",
    "mbert_df[\"commentText\"] = mbert_df[\"commentText\"].astype(str).str.strip().str.lower()\n",
    "lstm_df[\"commentText\"] = lstm_df[\"commentText\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Setup FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the request body model\n",
    "class SentenceInput(BaseModel):\n",
    "    sentence: str\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Harassment Detection API is running\"}\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(input_data: SentenceInput):\n",
    "    sent = input_data.sentence.strip().lower()\n",
    "    \n",
    "    # Match using 'commentText' column instead of non-existent 'sentence'\n",
    "    mbert_result = mbert_df[mbert_df[\"commentText\"] == sent]\n",
    "    lstm_result = lstm_df[lstm_df[\"commentText\"] == sent]\n",
    "    \n",
    "    if mbert_result.empty or lstm_result.empty:\n",
    "        raise HTTPException(status_code=404, detail=\"Sentence not found in predictions\")\n",
    "\n",
    "    return {\n",
    "        \"commentText\": input_data.sentence,\n",
    "        \"model_mbert\": int(mbert_result[\"predicted_label\"].values[0]),\n",
    "        \"model_lstm\": int(lstm_result[\"predicted_label\"].values[0])\n",
    "    }\n",
    "\n",
    "# Run the API server\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62739765-bbc1-4919-b331-7c6897de468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fuzzywuzzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddbb25-3b11-404f-9fbe-1cae03b05c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c7ef0-c145-4dbb-b4e9-73483026bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "# Allow running Uvicorn in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Setup FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Load mBERT model and tokenizer\n",
    "mbert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "mbert_model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Load LSTM model (assuming the model is saved as a PyTorch model file)\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        output = self.fc(hn[-1])\n",
    "        return output\n",
    "\n",
    "# Assuming the model is stored in 'lstm_model.pth'\n",
    "lstm_model = LSTMModel(input_size=300, hidden_size=128, output_size=2)  # Example params\n",
    "lstm_model.load_state_dict(torch.load(\"lstm_model.pth\"))\n",
    "lstm_model.eval()\n",
    "\n",
    "# Class for input data\n",
    "class SentenceInput(BaseModel):\n",
    "    sentence: str\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Harassment Detection API is running\"}\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(input_data: SentenceInput):\n",
    "    # Get the sentence from input\n",
    "    sent = input_data.sentence.strip()\n",
    "\n",
    "    # Process and predict with mBERT\n",
    "    mbert_input = mbert_tokenizer(sent, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        mbert_output = mbert_model(**mbert_input)\n",
    "    mbert_pred = torch.argmax(mbert_output.logits, dim=1).item()  # Predict the label\n",
    "\n",
    "    # Process and predict with LSTM (assuming sentence is tokenized and embedded)\n",
    "    # Here, we use dummy embedding for LSTM prediction, replace with real embeddings\n",
    "    sentence_embedding = torch.randn(1, 1, 300)  # Example random tensor, replace with actual embeddings\n",
    "    with torch.no_grad():\n",
    "        lstm_output = lstm_model(sentence_embedding)\n",
    "    lstm_pred = torch.argmax(lstm_output, dim=1).item()\n",
    "\n",
    "    return {\n",
    "        \"commentText\": input_data.sentence,\n",
    "        \"model_mbert\": mbert_pred,\n",
    "        \"model_lstm\": lstm_pred\n",
    "    }\n",
    "\n",
    "# Run the API\n",
    "uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba1cc13-c032-4e5c-929c-55a34d67a727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
